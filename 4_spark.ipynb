{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark\n",
      "  Using cached pyspark-3.5.3.tar.gz (317.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml): started\n",
      "  Building wheel for pyspark (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840672 sha256=57346ed42f665123fdac65a936685622896f1cb7afd545c9b49a6362fb488ef4\n",
      "  Stored in directory: c:\\users\\cfc\\appdata\\local\\pip\\cache\\wheels\\97\\f5\\c0\\947e2c0942b361ffe58651f36bd7f13772675b3863fd63d1b1\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import uuid\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from configs import kafka_config\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\n",
    "    'PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.1,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 pyspark-shell'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"KafkaStreaming\")\n",
    "         .master(\"local[*]\")\n",
    "         .config(\"spark.sql.debug.maxToStringFields\", \"200\")\n",
    "         .config(\"spark.sql.columnNameLengthThreshold\", \"200\")\n",
    "         .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alerts_df = spark.read.csv(\"./alerts_conditions.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------------+---------------+---------------+----+-------------+\n",
      "| id|humidity_min|humidity_max|temperature_min|temperature_max|code|      message|\n",
      "+---+------------+------------+---------------+---------------+----+-------------+\n",
      "|  1|           0|          40|           -999|           -999| 101| It's too dry|\n",
      "|  2|          60|         100|           -999|           -999| 102| It's too wet|\n",
      "|  3|        -999|        -999|           -300|             30| 103|It's too cold|\n",
      "|  4|        -999|        -999|             40|            300| 104| It's too hot|\n",
      "+---+------------+------------+---------------+---------------+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alerts_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "window_duration = \"1 minute\"\n",
    "sliding_interval = \"30 seconds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_config['bootstrap_servers'][0]) \\\n",
    "    .option(\"kafka.security.protocol\", \"SASL_PLAINTEXT\") \\\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    "    .option(\"kafka.sasl.jaas.config\",\n",
    "            'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"admin\" password=\"VawEzo1ikLtrA8Ug8THa\";') \\\n",
    "    .option(\"subscribe\", \"building_sensors_volodymyr17\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", \"300\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_schema = StructType([\n",
    "    StructField(\"sensor_id\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"temperature\", IntegerType(), True),\n",
    "    StructField(\"humidity\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "avg_stats = df.selectExpr(\"CAST(key AS STRING) AS key_deserialized\", \"CAST(value AS STRING) AS value_deserialized\", \"*\") \\\n",
    "    .drop('key', 'value') \\\n",
    "    .withColumnRenamed(\"key_deserialized\", \"key\") \\\n",
    "    .withColumn(\"value_json\", from_json(col(\"value_deserialized\"), json_schema)) \\\n",
    "    .withColumn(\"timestamp\", from_unixtime(col(\"value_json.timestamp\").cast(DoubleType())).cast(\"timestamp\")) \\\n",
    "    .withWatermark(\"timestamp\", \"10 seconds\") \\\n",
    "    .groupBy(window(col(\"timestamp\"), window_duration, sliding_interval)) \\\n",
    "    .agg(\n",
    "    avg(\"value_json.temperature\").alias(\"t_avg\"),\n",
    "    avg(\"value_json.humidity\").alias(\"h_avg\")\n",
    ") \\\n",
    "    .drop(\"topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_alerts = avg_stats.crossJoin(alerts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "valid_alerts = all_alerts \\\n",
    "    .where(\"t_avg > temperature_min AND t_avg < temperature_max\") \\\n",
    "    .unionAll(\n",
    "    all_alerts\n",
    "    .where(\"h_avg > humidity_min AND h_avg < humidity_max\")\n",
    ") \\\n",
    "    .withColumn(\"timestamp\", lit(str(datetime.datetime.now()))) \\\n",
    "    .drop(\"id\", \"humidity_min\", \"humidity_max\", \"temperature_min\", \"temperature_max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Для дебагінгу. Принт проміжного резульату.\n",
    "# displaying_df = valid_alerts.writeStream \\\n",
    "#     .trigger(processingTime='10 seconds') \\\n",
    "#     .outputMode(\"update\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start() \\\n",
    "#     .awaitTermination()\n",
    "\n",
    "uuid_udf = udf(lambda: str(uuid.uuid4()), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prepare_to_kafka_df = valid_alerts \\\n",
    "    .withColumn(\"key\", uuid_udf()) \\\n",
    "    .select(\n",
    "    col(\"key\"),\n",
    "    to_json(struct(col(\"window\"),\n",
    "                   col(\"t_avg\"),\n",
    "                   col(\"h_avg\"),\n",
    "                   col(\"code\"),\n",
    "                   col(\"message\"),\n",
    "                   col(\"timestamp\"))).alias(\"value\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для дебагінгу. Принт проміжного резульату.\n",
    "displaying_df = valid_alerts.writeStream \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start() \\\n",
    "    .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# query = prepare_to_kafka_df.writeStream \\\n",
    "#     .trigger(processingTime='30 seconds') \\\n",
    "#     .outputMode(\"update\") \\\n",
    "#     .format(\"kafka\") \\\n",
    "#     .option(\"kafka.bootstrap.servers\", \"77.81.230.104:9092\") \\\n",
    "#     .option(\"topic\", \"avg_alerts\") \\\n",
    "#     .option(\"kafka.security.protocol\", \"SASL_PLAINTEXT\") \\\n",
    "#     .option(\"kafka.sasl.mechanism\", \"PLAIN\") \\\n",
    "#     .option(\"kafka.sasl.jaas.config\",\n",
    "#             \"org.apache.kafka.common.security.plain.PlainLoginModule required username='admin' password='VawEzo1ikLtrA8Ug8THa';\") \\\n",
    "#     .option(\"checkpointLocation\", \"/tmp/checkpoints-7\") \\\n",
    "#     .start() \\\n",
    "#     .awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
